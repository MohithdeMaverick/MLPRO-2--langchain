# -*- coding: utf-8 -*-
"""MLPRO-2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eer28JojB_bT1kzNmOS_jEvORetwfzoN
"""

!pip install langchain==0.0.284

!pip install faiss-cpu==1.7.4

!pip install protobuf==3.19.0

!pip install tiktoken==0.4.0

!pip install python-dotenv==1.0.0

from langchain.llms import GooglePalm

llm=GooglePalm(google_api_key="AIzaSyC8EbOFyoSTxs9ksLI4KfPltYV6-5je-kI",temperature=0.7)

print(llm("who are you"))

from langchain.document_loaders.csv_loader import CSVLoader
#loader=CSVLoader(file_path='codebasics_faqs.csv',source_column="prompt")
loader = CSVLoader(file_path='codebasics_faqs.csv', source_column="prompt", encoding="ISO-8859-1")

data=loader.load()
print(data)

# Commented out IPython magic to ensure Python compatibility.
# !pip install sentence-transformers==2.2.2

from langchain.embeddings import HuggingFaceInstructEmbeddings
# %pip install llama-index-embeddings-huggingface
# %pip install llama-index-embeddings-instructor
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
embed_model = HuggingFaceEmbedding(model_name="BAAI/bge-small-en-v1.5")

from langchain.embeddings import HuggingFaceEmbeddings
embeddings = HuggingFaceEmbeddings()

from langchain.vectorstores import FAISS

db = FAISS.from_documents(data, embeddings)

retrive=db.as_retriever()
retrive.get_relevant_documents("how about job placement support?")

from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
prompt_template = """Given the following context and a question, generate an answer based on this context only.
    In the answer try to provide as much text as possible from "response" section in the source document context without making much changes.
    If the answer is not found in the context, kindly state "I don't know." Don't try to make up an answer.

    CONTEXT: {context}

    QUESTION: {question}"""

PROMPT = PromptTemplate(
    template=prompt_template, input_variables=["context", "question"]
)

chain = RetrievalQA.from_chain_type(llm=llm,chain_type="stuff", retriever=retrive, input_key="query", return_source_documents=True,chain_type_kwargs={"prompt": PROMPT})

chain("are you offering langchain course?")